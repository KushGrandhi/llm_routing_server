# LLM Gateway Model Registry
# Add/remove models here without redeploying the server
# Use POST /v1/models/reload to hot-reload this config

models:
  # OpenAI Models
  - name: "gpt-4o"
    provider: "openai"
    model_id: "gpt-4o"
    
  - name: "gpt-4o-mini"
    provider: "openai"
    model_id: "gpt-4o-mini"

  # Anthropic Claude Models
  - name: "claude-sonnet"
    provider: "anthropic"
    model_id: "claude-sonnet-4-20250514"
    
  - name: "claude-haiku"
    provider: "anthropic"
    model_id: "claude-3-5-haiku-20241022"

  # Google Gemini Models (Free tier available)
  - name: "gemini-flash"
    provider: "gemini"
    model_id: "gemini/gemini-2.0-flash"
    
  - name: "gemini-pro"
    provider: "gemini"
    model_id: "gemini/gemini-1.5-pro"

  # Groq Models (Fast inference, free tier)
  - name: "llama-groq"
    provider: "groq"
    model_id: "groq/llama-3.3-70b-versatile"
    
  - name: "mixtral-groq"
    provider: "groq"
    model_id: "groq/mixtral-8x7b-32768"

  # HuggingFace Models
  - name: "mistral-hf"
    provider: "huggingface"
    model_id: "huggingface/mistralai/Mistral-7B-Instruct-v0.2"

  # Custom Self-Hosted Models (OpenAI-compatible endpoints)
  # Uncomment and configure when you have a self-hosted model
  # - name: "local-llama"
  #   provider: "custom_openai"
  #   model_id: "llama-2-7b"
  #   api_base: "http://localhost:8080/v1"

# Default model to use when none specified
default_model: "gemini-flash"

